{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2d5de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c28d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPTNeoXForCausalLM, get_cosine_schedule_with_warmup, TrainingArguments\n",
    "from huggingface_hub import hf_hub_download\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, SequentialSampler\n",
    "from torch.optim import Adam\n",
    "from PythiaBinaryDataset import PythiaBinaryDataset\n",
    "from NoShuffleTrainer import NoShuffleTrainer\n",
    "from SynonymCrossEntropy import SynonymCrossEntropy\n",
    "import wandb\n",
    "import yaml\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "def load_config(path: str):\n",
    "    try:\n",
    "        with open(path, 'r') as config:\n",
    "            config = yaml.safe_load(config) #returns dict btw\n",
    "            return config\n",
    "    except FileNotFoundError:\n",
    "        print(\"Config File Not Found. Check your path!!\")\n",
    "\n",
    "config_dict = load_config('/Users/nathan/Documents/Development/project2/pythia_160m_deduped_config.yaml')\n",
    "\n",
    "wandb.init(\n",
    "    project=\"pythia-160m-test-training-run\", \n",
    "    name=\"Test-Run-01\",\n",
    "    config=config_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f41a43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iters = config_dict['train-iters']\n",
    "seq_len = config_dict['seq-length']\n",
    "global_batch_size = 1024 # attempts to simulate using micro batches\n",
    "\n",
    "micro_batch_size = 8 # done for grad accum\n",
    "grad_accum_steps = global_batch_size // micro_batch_size\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "model = GPTNeoXForCausalLM.from_pretrained(\"EleutherAI/pythia-160m-deduped\", revision=\"step0\", cache_dir=\"./pythia-160m-deduped/step0\")\n",
    "#tokenizer unnecessary, i think document.bin comes preprocessed with the targets idk\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "    \n",
    "optimizer = Adam(\n",
    "    model.parameters(), \n",
    "    lr = config_dict['optimizer']['params']['lr'], \n",
    "    betas = config_dict['optimizer']['params']['betas'], \n",
    "    eps = config_dict['optimizer']['params']['eps'], \n",
    "    weight_decay = config_dict['weight-decay']\n",
    "    )\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_training_steps = train_iters,\n",
    "    num_warmup_steps = int(train_iters * config_dict['warmup'])\n",
    ")\n",
    "\n",
    "dataset = PythiaBinaryDataset(\"./pythia_data/document.bin\", seq_len)\n",
    "\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./checkpoints\",\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=grad_accum_steps,\n",
    "    max_steps=train_iters,  \n",
    "    logging_steps=1,\n",
    "    report_to=\"wandb\",\n",
    "    save_steps=1000,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    bf16=torch.mps.is_available()\n",
    ")\n",
    "\n",
    "trainer = NoShuffleTrainer(\n",
    "    model=model, \n",
    "    args=training_arguments,\n",
    "    train_dataset=dataset,\n",
    "    optimizers=(optimizer, scheduler),\n",
    "    # compute_loss_func = SynonymCrossEntropy()\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
