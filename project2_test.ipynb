{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46113876",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, Trainer, AutoModelForCausalLM\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'EleutherAI/gpt-neox-20b',\n",
    "    dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neox-20b')\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "embed_mat = model.get_input_embeddings().weight.to(device)\n",
    "embed_mat.to(device)\n",
    "del model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b477c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('openai-community/gpt2', dtype=torch.float16)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('openai-community/gpt2')\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = '<|pad|>'\n",
    "\n",
    "tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1514886",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65947006",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d5c537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from hf_olmo import OLMoTokenizerFast\n",
    "\n",
    "# def create_neighbor_lookup_table(embed_matrix, num_neighbors):\n",
    "#     seq_len = embed_matrix.shape[0]\n",
    "#     scaffold = []\n",
    "#     for token_idx in range(seq_len):\n",
    "#         target = embed_matrix[token_idx]\n",
    "#         norms = torch.norm(embed_matrix - target, dim=-1, p=2)\n",
    "#         norms[token_idx] = float('inf')\n",
    "#         knn = norms.topk(num_neighbors, largest=False, sorted=True) # chooses smallest, sorts from smallest to largest\n",
    "#         distances, indexes = knn[0], knn[1]\n",
    "#         # distance_weights = F.softmax(-distances, dim=-1) # for weighting by distance\n",
    "#         # neg_log_likelihoods = -torch.log(distance_weights) * distance_weights\n",
    "#         # assert neg_log_likelihoods.shape[0] == num_neighbors, \"check neg_log_likelihood/weights sizes\"\n",
    "#         scaffold.append([(distances, indexes)])\n",
    "#     return scaffold\n",
    "\n",
    "def create_neighbor_lookup_table(embed_matrix, k): # selects neighbors based on cos similarity and l2 distance now. we have achieved a dynamic set!\n",
    "    with torch.no_grad():\n",
    "        #cos vers\n",
    "        cosine_similarity_scores = F.normalize(embed_matrix, p=2, dim=-1) @ (F.normalize(embed_matrix, p=2, dim=-1)).T\n",
    "        cosine_similarity_scores = torch.clamp(cosine_similarity_scores, min=-1.0 + 1e-7, max=1.0-1e-7)\n",
    "        angles = torch.acos(cosine_similarity_scores)\n",
    "        angles.fill_diagonal_(float('inf')) #angle of vector with itself is 0. would be selected lmao.\n",
    "        cos_sorted = torch.topk(torch.acos(cosine_similarity_scores), k=k, dim=-1, largest=False, sorted=True)\n",
    "        cos_angles, cos_indices = cos_sorted[0], cos_sorted[1]\n",
    "    \n",
    "        # embed_matrix = embed_matrix.unsqueeze(0) # (1, seq_len, dim)\n",
    "        # dists = torch.cdist(embed_matrix, embed_matrix, p=2).squeeze() # cdist necessitates batch dim for som e reason. creates it, does the cdist, then removes the auxiliary dim.\n",
    "        # # distance of a vector with itself is zero. fill it with float('inf') so that topk doesnl't select it\n",
    "        # dists.fill_diagonal_(float('inf'))\n",
    "        # sorted = torch.sort(dists, dim=-1)\n",
    "        # l2_indices = sorted[1]\n",
    "        # l2_distances = sorted[0]\n",
    "\n",
    "        # filter = cos_indicies == l2_indices\n",
    "\n",
    "        return cos_angles, cos_indices\n",
    "    \n",
    "#ver 3\n",
    "def create_lookup_table(model, num_neighbors): #perform on h100 or higher for improved efficacy.\n",
    "\n",
    "    tokenizer = OLMoTokenizerFast.from_pretrained(\"allenai/OLMo-1B\")\n",
    "    sorted_vocab = sorted(tokenizer.get_vocab().items(), key=lambda item: item[1]) #list of tuples (token, id)\n",
    "    dict_sorted_vocab = dict(sorted_vocab)\n",
    "    token_strings = [pair[0] for pair in sorted_vocab]\n",
    "\n",
    "    similarity_list = [] #will store ordered cos sims\n",
    "    index_list = [] # will store respective indices\n",
    "    for token in token_strings:\n",
    "        similarities = model.similarity(model.encode_query(token), model.encode_document(token_strings))\n",
    "        similarities[dict_sorted_vocab[token]] = float('-inf') #when passed into toipk, it will be not selected. we want the tokens with largest cos sim.\n",
    "        knn = torch.topk(similarities, k=num_neighbors, largest=True, sorted=True) #greatest to least\n",
    "        similarity_list.append(list(knn[0]))\n",
    "        index_list.append(list(knn[1]))\n",
    "\n",
    "    return torch.tensor(similarity_list), torch.tensor(index_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39be4158",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scaffold = create_neighbor_lookup_table(embed_mat, 1)\n",
    "\n",
    "test = torch.ones((3,2))\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83560812",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_dict = model.state_dict()\n",
    "\n",
    "embed_mat = model_state_dict[list(model_state_dict.keys())[0]]\n",
    "embed_mat.shape\n",
    "embed_mat.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c332092f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "test = torch.ones((3))\n",
    "test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95329712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5525fe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "rand_idx = int((torch.rand((1,)) *  embed_mat.shape[0]).item())\n",
    "\n",
    "\n",
    "test_target = embed_mat[rand_idx]\n",
    "dist = torch.norm(embed_mat - test_target, dim=1, p=None)\n",
    "dist[rand_idx] = float('inf')\n",
    "knn = dist.topk(2, largest=False, sorted=True)\n",
    "\n",
    "knn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5258cdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([10, 3, 7, 20, 15])\n",
    "\n",
    "values, indices = torch.topk(x, k=3, sorted=True)\n",
    "\n",
    "print(\"input:\", x)\n",
    "print(\"topk values:\", values)\n",
    "print(\"topk indices:\", indices)\n",
    "print(\"values via indices:\", x[indices])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e09ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load EmbeddingGemma model and tokenizer\n",
    "model_name = \"google/embeddinggemma-300m\"\n",
    "other_model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Get embedding matrix\n",
    "embed_matrix = other_model.get_input_embeddings().weight\n",
    "\n",
    "# Convert to numpy\n",
    "if torch.is_tensor(embed_matrix):\n",
    "    embed_matrix_np = embed_matrix.detach().cpu().float().numpy()\n",
    "else:\n",
    "    embed_matrix_np = np.array(embed_matrix)\n",
    "\n",
    "print(f\"Embedding matrix shape: {embed_matrix_np.shape}\")\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a87871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_neighbors(token, k=10):\n",
    "    \"\"\"\n",
    "    Find k nearest neighbors to a given token based on cosine similarity.\n",
    "    \n",
    "    Args:\n",
    "        token: Input token (string)\n",
    "        k: Number of nearest neighbors to return\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples: (neighbor_token, cosine_similarity, angle_degrees, l2_distance)\n",
    "    \"\"\"\n",
    "    # Encode the token\n",
    "    token_ids = tokenizer.encode(token, add_special_tokens=False)\n",
    "    \n",
    "    if len(token_ids) == 0:\n",
    "        return f\"Could not encode token: {token}\"\n",
    "    \n",
    "    # Use the first token if multiple tokens are generated\n",
    "    token_id = token_ids[0]\n",
    "    \n",
    "    # Get the embedding for this token\n",
    "    query_embedding = embed_matrix_np[token_id].reshape(1, -1)\n",
    "    \n",
    "    # Compute cosine similarities with all tokens\n",
    "    similarities = cosine_similarity(query_embedding, embed_matrix_np)[0]\n",
    "    \n",
    "    # Get top k+1 indices (including the token itself)\n",
    "    top_indices = np.argsort(similarities)[::-1][:k+1]\n",
    "    \n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        if idx == token_id:\n",
    "            continue  # Skip the token itself\n",
    "            \n",
    "        neighbor_token = tokenizer.decode([idx])\n",
    "        cos_sim = similarities[idx]\n",
    "        \n",
    "        # Calculate angle in degrees: angle = arccos(cosine_similarity)\n",
    "        angle_rad = np.arccos(np.clip(cos_sim, -1.0, 1.0))\n",
    "        angle_deg = np.degrees(angle_rad)\n",
    "        \n",
    "        # Calculate L2 distance\n",
    "        l2_dist = np.linalg.norm(query_embedding - embed_matrix_np[idx])\n",
    "        \n",
    "        results.append((neighbor_token, cos_sim, angle_deg, l2_dist))\n",
    "        \n",
    "        if len(results) == k:\n",
    "            break\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def find_nearest_neighbors_l2(token, k=10):\n",
    "    \"\"\"\n",
    "    Find k nearest neighbors to a given token based on L2 distance.\n",
    "    \n",
    "    Args:\n",
    "        token: Input token (string)\n",
    "        k: Number of nearest neighbors to return\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples: (neighbor_token, angle_degrees, l2_distance)\n",
    "    \"\"\"\n",
    "    # Encode the token\n",
    "    token_ids = tokenizer.encode(token, add_special_tokens=False)\n",
    "    \n",
    "    if len(token_ids) == 0:\n",
    "        return f\"Could not encode token: {token}\"\n",
    "    \n",
    "    # Use the first token if multiple tokens are generated\n",
    "    token_id = token_ids[0]\n",
    "    \n",
    "    # Get the embedding for this token\n",
    "    query_embedding = embed_matrix_np[token_id]\n",
    "    \n",
    "    # Compute L2 distances with all tokens\n",
    "    l2_distances = np.linalg.norm(embed_matrix_np - query_embedding, axis=1)\n",
    "    \n",
    "    # Get top k+1 indices (including the token itself)\n",
    "    top_indices = np.argsort(l2_distances)[:k+1]\n",
    "    \n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        if idx == token_id:\n",
    "            continue  # Skip the token itself\n",
    "            \n",
    "        neighbor_token = tokenizer.decode([idx])\n",
    "        l2_dist = l2_distances[idx]\n",
    "        \n",
    "        # Calculate cosine similarity and angle\n",
    "        neighbor_embedding = embed_matrix_np[idx]\n",
    "        cos_sim = np.dot(query_embedding, neighbor_embedding) / (\n",
    "            np.linalg.norm(query_embedding) * np.linalg.norm(neighbor_embedding)\n",
    "        )\n",
    "        \n",
    "        # Calculate angle in degrees: angle = arccos(cosine_similarity)\n",
    "        angle_rad = np.arccos(np.clip(cos_sim, -1.0, 1.0))\n",
    "        angle_deg = np.degrees(angle_rad)\n",
    "        \n",
    "        results.append((neighbor_token, angle_deg, l2_dist))\n",
    "        \n",
    "        if len(results) == k:\n",
    "            break\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def plot_neighbors_l2_vs_angle(neighbors, title=None, save_path=None, show=True):\n",
    "    \"\"\"Plot L2 distance (x) vs angle in degrees (y) for a neighbors list.\n",
    "\n",
    "    neighbors: list of (token_str, angle_deg, l2_distance)\n",
    "    \"\"\"\n",
    "    if not neighbors:\n",
    "        raise ValueError(\"neighbors list is empty\")\n",
    "\n",
    "    tokens = [n[0] for n in neighbors]\n",
    "    angles = np.array([float(n[1]) for n in neighbors])\n",
    "    l2 = np.array([float(n[2]) for n in neighbors])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7, 5))\n",
    "    sc = ax.scatter(l2, angles, c=l2, cmap=\"viridis\", s=80, edgecolors=\"k\", alpha=0.9)\n",
    "\n",
    "    for i, tok in enumerate(tokens):\n",
    "        ax.annotate(str(tok), (l2[i], angles[i]), xytext=(6, 4), textcoords=\"offset points\", fontsize=8)\n",
    "\n",
    "    ax.set_xlabel(\"L2 distance\")\n",
    "    ax.set_ylabel(\"Angle (degrees)\")\n",
    "    ax.set_title(title or \"L2 distance vs Angle\")\n",
    "    cb = fig.colorbar(sc, ax=ax)\n",
    "    cb.set_label(\"L2 distance\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=200)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def plot_neighbors_l2_vs_angle_cos(neighbors, title=None, save_path=None, show=True):\n",
    "    \"\"\"Plot L2 distance (x) vs angle in degrees (y) for a neighbors list.\n",
    "\n",
    "    neighbors: list of (token_str, cos_sim, angle_deg, l2_distance)\n",
    "    \"\"\"\n",
    "    if not neighbors:\n",
    "        raise ValueError(\"neighbors list is empty\")\n",
    "\n",
    "    tokens = [n[0] for n in neighbors]\n",
    "    angles = np.array([float(n[2]) for n in neighbors])\n",
    "    l2 = np.array([float(n[3]) for n in neighbors])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7, 5))\n",
    "    sc = ax.scatter(l2, angles, c=l2, cmap=\"viridis\", s=80, edgecolors=\"k\", alpha=0.9)\n",
    "\n",
    "    for i, tok in enumerate(tokens):\n",
    "        ax.annotate(str(tok), (l2[i], angles[i]), xytext=(6, 4), textcoords=\"offset points\", fontsize=8)\n",
    "\n",
    "    ax.set_xlabel(\"L2 distance\")\n",
    "    ax.set_ylabel(\"Angle (degrees)\")\n",
    "    ax.set_title(title or \"L2 distance vs Angle\")\n",
    "    cb = fig.colorbar(sc, ax=ax)\n",
    "    cb.set_label(\"L2 distance\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=200)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "    return fig, ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89aab589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nearest neighbors for token (using cosine similarity): 'to'\n",
      "\n",
      "Token                Cosine Sim   Angle (°)    L2 Distance \n",
      "------------------------------------------------------------\n",
      "To                   0.668752     48.0292      1.2046      \n",
      " to                  0.626267     51.2248      1.2688      \n",
      "TO                   0.617295     51.8811      1.3679      \n",
      " To                  0.598961     53.2045      1.3048      \n",
      " TO                  0.552411     56.4674      1.3444      \n",
      "the                  0.508897     59.4096      1.4505      \n",
      "ta                   0.495218     60.3159      1.6385      \n",
      "tos                  0.491079     60.5885      1.7003      \n",
      "of                   0.446269     63.4954      1.5997      \n",
      "so                   0.446135     63.5040      1.7486      \n",
      "for                  0.420356     65.1429      1.6808      \n",
      "with                 0.419574     65.1923      1.6739      \n",
      "te                   0.416712     65.3728      1.7772      \n",
      "and                  0.411123     65.7246      1.6307      \n",
      "t                    0.410785     65.7459      1.7881      \n",
      "ten                  0.410490     65.7644      1.8560      \n",
      "do                   0.408089     65.9152      1.8092      \n",
      "ti                   0.407782     65.9344      1.8373      \n",
      "ts                   0.395695     66.6907      1.8521      \n",
      "ton                  0.393956     66.7991      1.8781      \n",
      "that                 0.388320     67.1500      1.6937      \n",
      "from                 0.385921     67.2990      1.7414      \n",
      "no                   0.384227     67.4042      1.8215      \n",
      "on                   0.383860     67.4270      1.7414      \n",
      "sto                  0.381556     67.5699      1.8705      \n",
      "tr                   0.379811     67.6781      1.8370      \n",
      "by                   0.378433     67.7634      1.8560      \n",
      "into                 0.378040     67.7876      1.9100      \n",
      "be                   0.376236     67.8992      1.8211      \n",
      "de                   0.368085     68.4025      1.8123      \n",
      "in                   0.367750     68.4231      1.7405      \n",
      "re                   0.365102     68.5862      1.7962      \n",
      "mo                   0.363818     68.6652      1.8835      \n",
      "ty                   0.363073     68.7109      1.9538      \n",
      "top                  0.362145     68.7680      1.8791      \n",
      "this                 0.356533     69.1126      1.7992      \n",
      "more                 0.352365     69.3680      1.8864      \n",
      "tom                  0.351107     69.4450      1.9379      \n",
      "tt                   0.350893     69.4580      1.9007      \n",
      "टू                   0.350669     69.4717      1.6932      \n",
      "th                   0.348433     69.6085      1.9126      \n",
      "tot                  0.346661     69.7168      1.9398      \n",
      "ter                  0.345839     69.7670      1.9088      \n",
      "co                   0.343901     69.8853      1.8468      \n",
      "tem                  0.341645     70.0228      1.9398      \n",
      "as                   0.340121     70.1158      1.8161      \n",
      "sp                   0.339981     70.1243      1.8683      \n",
      "tes                  0.339426     70.1581      2.0001      \n",
      "tu                   0.339395     70.1600      1.9958      \n",
      "or                   0.339059     70.1804      1.8630      \n"
     ]
    }
   ],
   "source": [
    "token = \"to\"\n",
    "\n",
    "neighbors_cos = find_nearest_neighbors(token, k=50)\n",
    "\n",
    "print(f\"\\nNearest neighbors for token (using cosine similarity): '{token}'\\n\")\n",
    "print(f\"{'Token':<20} {'Cosine Sim':<12} {'Angle (°)':<12} {'L2 Distance':<12}\")\n",
    "print(\"-\" * 60)\n",
    "for neighbor, cos_sim, angle, l2_dist in neighbors_cos:\n",
    "    print(f\"{neighbor:<20} {cos_sim:<12.6f} {angle:<12.4f} {l2_dist:<12.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76afd6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    fig, ax = plot_neighbors_l2_vs_angle_cos(neighbors_cos, title=f\"Neighbors | cos_similarity for '{token}'\")\n",
    "except NameError:\n",
    "    print(\"`neighbors` or `token` not defined in notebook; call find_nearest_neighbors_l2 first and re-run this cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ee1008",
   "metadata": {},
   "source": [
    "Appending new objectives it would be as if it were optimizing over a separate sequence. Since they both yield similar neighbors when k ≅ 5 we can choose either euclidean distance or cosine similarity arbitrarily. However, if we would like to glean better results, we would have to perform this empirically. Run the experiment both on euclidean and cosine and see which one works better.\n",
    "\n",
    " In order to reconcile both metrics, as cosine similarity or euclidean distance may be more informative depending on the context, we can choose neighbors in which both metrics align. (meaning that if one increases and the other also increases (doesn't decrease) and the token name is the same, keep the token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270c4e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: mps\n",
      "Loading openai-community/gpt2...\n",
      "\n",
      "Generating 15 tokens for prompt: 'Given that '\n",
      "\n",
      "Final Text: Given that  it's a task for museums, I decided to design an interactive representations\n",
      "\n",
      "------------------------------\n",
      "Step 1: Chosen -> ' '\n",
      "    Token: '\\xa0'          | Prob: 0.4823 | Perplexity: 2.0733\n",
      "    Token: '________'      | Prob: 0.0326 | Perplexity: 30.6886\n",
      "    Token: 'vern'          | Prob: 0.0278 | Perplexity: 35.9865\n",
      "    Token: '____'          | Prob: 0.0218 | Perplexity: 45.9751\n",
      "    Token: '_____'         | Prob: 0.0171 | Perplexity: 58.4956\n",
      "    Token: 'ive'           | Prob: 0.0143 | Perplexity: 70.1315\n",
      "    Token: '_______'       | Prob: 0.0136 | Perplexity: 73.3011\n",
      "    Token: '�'             | Prob: 0.0121 | Perplexity: 82.8051\n",
      "    Token: 'iced'          | Prob: 0.0120 | Perplexity: 83.4721\n",
      "    Token: '�'             | Prob: 0.0117 | Perplexity: 85.4473\n",
      "Step 2: Chosen -> 'it'\n",
      "    Token: 'the'           | Prob: 0.1345 | Perplexity: 7.4353\n",
      "    Token: 'it'            | Prob: 0.0433 | Perplexity: 23.1035\n",
      "    Token: 'I'             | Prob: 0.0335 | Perplexity: 29.8691\n",
      "    Token: 'we'            | Prob: 0.0330 | Perplexity: 30.3092\n",
      "    Token: 'you'           | Prob: 0.0322 | Perplexity: 31.0571\n",
      "    Token: 'there'         | Prob: 0.0220 | Perplexity: 45.5374\n",
      "    Token: 'this'          | Prob: 0.0219 | Perplexity: 45.6605\n",
      "    Token: 'a'             | Prob: 0.0205 | Perplexity: 48.7971\n",
      "    Token: 'he'            | Prob: 0.0145 | Perplexity: 68.8522\n",
      "    Token: 'they'          | Prob: 0.0136 | Perplexity: 73.4692\n",
      "Step 3: Chosen -> ''s'\n",
      "    Token: ' is'           | Prob: 0.2898 | Perplexity: 3.4511\n",
      "    Token: \"'s\"            | Prob: 0.2016 | Perplexity: 4.9597\n",
      "    Token: ' was'          | Prob: 0.1081 | Perplexity: 9.2519\n",
      "    Token: ' has'          | Prob: 0.0454 | Perplexity: 22.0396\n",
      "    Token: ' would'        | Prob: 0.0225 | Perplexity: 44.5040\n",
      "    Token: ' seems'        | Prob: 0.0163 | Perplexity: 61.2582\n",
      "    Token: ' can'          | Prob: 0.0144 | Perplexity: 69.5578\n",
      "    Token: ' takes'        | Prob: 0.0129 | Perplexity: 77.7834\n",
      "    Token: ' will'         | Prob: 0.0119 | Perplexity: 84.1309\n",
      "    Token: ' took'         | Prob: 0.0111 | Perplexity: 90.2558\n",
      "Step 4: Chosen -> ' a'\n",
      "    Token: ' not'          | Prob: 0.1208 | Perplexity: 8.2786\n",
      "    Token: ' a'            | Prob: 0.0790 | Perplexity: 12.6661\n",
      "    Token: ' been'         | Prob: 0.0361 | Perplexity: 27.7161\n",
      "    Token: ' possible'     | Prob: 0.0339 | Perplexity: 29.5298\n",
      "    Token: ' the'          | Prob: 0.0286 | Perplexity: 35.0055\n",
      "    Token: ' hard'         | Prob: 0.0211 | Perplexity: 47.4868\n",
      "    Token: '\\xa0'          | Prob: 0.0175 | Perplexity: 57.2625\n",
      "    Token: ' an'           | Prob: 0.0155 | Perplexity: 64.6902\n",
      "    Token: ' still'        | Prob: 0.0129 | Perplexity: 77.3885\n",
      "    Token: ' only'         | Prob: 0.0129 | Perplexity: 77.5416\n",
      "Step 5: Chosen -> ' task'\n",
      "    Token: ' very'         | Prob: 0.0295 | Perplexity: 33.8823\n",
      "    Token: ' bit'          | Prob: 0.0222 | Perplexity: 45.1068\n",
      "    Token: '\\xa0'          | Prob: 0.0138 | Perplexity: 72.2727\n",
      "    Token: ' good'         | Prob: 0.0136 | Perplexity: 73.5622\n",
      "    Token: ' lot'          | Prob: 0.0129 | Perplexity: 77.5148\n",
      "    Token: ' pretty'       | Prob: 0.0121 | Perplexity: 82.4059\n",
      "    Token: ' small'        | Prob: 0.0113 | Perplexity: 88.5639\n",
      "    Token: ' long'         | Prob: 0.0113 | Perplexity: 88.8807\n",
      "    Token: ' fairly'       | Prob: 0.0103 | Perplexity: 97.2699\n",
      "    Token: ' little'       | Prob: 0.0093 | Perplexity: 107.0479\n",
      "Step 6: Chosen -> ' for'\n",
      "    Token: ' that'         | Prob: 0.2116 | Perplexity: 4.7256\n",
      "    Token: ' of'           | Prob: 0.0853 | Perplexity: 11.7262\n",
      "    Token: ' to'           | Prob: 0.0565 | Perplexity: 17.6983\n",
      "    Token: ' for'          | Prob: 0.0552 | Perplexity: 18.1286\n",
      "    Token: ' I'            | Prob: 0.0364 | Perplexity: 27.4681\n",
      "    Token: ','             | Prob: 0.0318 | Perplexity: 31.4640\n",
      "    Token: ' which'        | Prob: 0.0211 | Perplexity: 47.3930\n",
      "    Token: ' in'           | Prob: 0.0205 | Perplexity: 48.8571\n",
      "    Token: ' requiring'    | Prob: 0.0203 | Perplexity: 49.3611\n",
      "    Token: ' with'         | Prob: 0.0199 | Perplexity: 50.1866\n",
      "Step 7: Chosen -> ' museums'\n",
      "    Token: ' the'          | Prob: 0.1645 | Perplexity: 6.0786\n",
      "    Token: ' a'            | Prob: 0.0895 | Perplexity: 11.1784\n",
      "    Token: ' me'           | Prob: 0.0543 | Perplexity: 18.4188\n",
      "    Token: ' us'           | Prob: 0.0312 | Perplexity: 32.0460\n",
      "    Token: ' you'          | Prob: 0.0244 | Perplexity: 40.9184\n",
      "    Token: '\\xa0'          | Prob: 0.0227 | Perplexity: 44.0766\n",
      "    Token: ' each'         | Prob: 0.0190 | Perplexity: 52.5580\n",
      "    Token: ' an'           | Prob: 0.0184 | Perplexity: 54.3278\n",
      "    Token: ' both'         | Prob: 0.0182 | Perplexity: 54.9992\n",
      "    Token: ' all'          | Prob: 0.0112 | Perplexity: 89.3959\n",
      "Step 8: Chosen -> ','\n",
      "    Token: ' to'           | Prob: 0.4809 | Perplexity: 2.0793\n",
      "    Token: ' and'          | Prob: 0.1888 | Perplexity: 5.2973\n",
      "    Token: ','             | Prob: 0.1450 | Perplexity: 6.8976\n",
      "    Token: ' that'         | Prob: 0.0233 | Perplexity: 42.9189\n",
      "    Token: ' in'           | Prob: 0.0119 | Perplexity: 84.3054\n",
      "    Token: ' of'           | Prob: 0.0113 | Perplexity: 88.7553\n",
      "    Token: ' like'         | Prob: 0.0074 | Perplexity: 134.8780\n",
      "    Token: ' not'          | Prob: 0.0065 | Perplexity: 154.6619\n",
      "    Token: '.'             | Prob: 0.0060 | Perplexity: 166.3082\n",
      "    Token: ' ('            | Prob: 0.0058 | Perplexity: 173.6538\n",
      "Step 9: Chosen -> ' I'\n",
      "    Token: ' it'           | Prob: 0.0803 | Perplexity: 12.4486\n",
      "    Token: ' I'            | Prob: 0.0556 | Perplexity: 17.9765\n",
      "    Token: ' we'           | Prob: 0.0497 | Perplexity: 20.1170\n",
      "    Token: ' the'          | Prob: 0.0456 | Perplexity: 21.9362\n",
      "    Token: ' and'          | Prob: 0.0361 | Perplexity: 27.6674\n",
      "    Token: ' museums'      | Prob: 0.0358 | Perplexity: 27.9542\n",
      "    Token: ' they'         | Prob: 0.0296 | Perplexity: 33.8176\n",
      "    Token: ' not'          | Prob: 0.0179 | Perplexity: 55.7206\n",
      "    Token: ' but'          | Prob: 0.0152 | Perplexity: 65.5948\n",
      "    Token: '\\xa0'          | Prob: 0.0148 | Perplexity: 67.6930\n",
      "Step 10: Chosen -> ' decided'\n",
      "    Token: \"'m\"            | Prob: 0.1166 | Perplexity: 8.5791\n",
      "    Token: ' decided'      | Prob: 0.0550 | Perplexity: 18.1880\n",
      "    Token: \"'ve\"           | Prob: 0.0496 | Perplexity: 20.1769\n",
      "    Token: \"'d\"            | Prob: 0.0472 | Perplexity: 21.1917\n",
      "    Token: ' thought'      | Prob: 0.0447 | Perplexity: 22.3901\n",
      "    Token: ' think'        | Prob: 0.0414 | Perplexity: 24.1375\n",
      "    Token: \"'ll\"           | Prob: 0.0387 | Perplexity: 25.8334\n",
      "    Token: ' wanted'       | Prob: 0.0386 | Perplexity: 25.8926\n",
      "    Token: ' can'          | Prob: 0.0377 | Perplexity: 26.5030\n",
      "    Token: ' have'         | Prob: 0.0236 | Perplexity: 42.2923\n",
      "Step 11: Chosen -> ' to'\n",
      "    Token: ' to'           | Prob: 0.8537 | Perplexity: 1.1713\n",
      "    Token: ' that'         | Prob: 0.0373 | Perplexity: 26.8440\n",
      "    Token: ' I'            | Prob: 0.0306 | Perplexity: 32.7130\n",
      "    Token: ' it'           | Prob: 0.0185 | Perplexity: 53.9731\n",
      "    Token: ' not'          | Prob: 0.0119 | Perplexity: 83.7183\n",
      "    Token: '\\xa0'          | Prob: 0.0041 | Perplexity: 242.8289\n",
      "    Token: ' on'           | Prob: 0.0033 | Perplexity: 299.9401\n",
      "    Token: ' the'          | Prob: 0.0033 | Perplexity: 304.4851\n",
      "    Token: ' this'         | Prob: 0.0028 | Perplexity: 353.8657\n",
      "    Token: ','             | Prob: 0.0021 | Perplexity: 486.4429\n",
      "Step 12: Chosen -> ' design'\n",
      "    Token: ' do'           | Prob: 0.0633 | Perplexity: 15.8056\n",
      "    Token: ' take'         | Prob: 0.0585 | Perplexity: 17.0907\n",
      "    Token: ' make'         | Prob: 0.0472 | Perplexity: 21.1891\n",
      "    Token: ' create'       | Prob: 0.0470 | Perplexity: 21.2782\n",
      "    Token: ' go'           | Prob: 0.0351 | Perplexity: 28.4752\n",
      "    Token: ' try'          | Prob: 0.0314 | Perplexity: 31.8756\n",
      "    Token: ' use'          | Prob: 0.0304 | Perplexity: 32.9331\n",
      "    Token: ' write'        | Prob: 0.0297 | Perplexity: 33.6899\n",
      "    Token: ' give'         | Prob: 0.0276 | Perplexity: 36.1882\n",
      "    Token: ' look'         | Prob: 0.0205 | Perplexity: 48.6809\n",
      "Step 13: Chosen -> ' an'\n",
      "    Token: ' a'            | Prob: 0.3938 | Perplexity: 2.5396\n",
      "    Token: ' an'           | Prob: 0.0923 | Perplexity: 10.8353\n",
      "    Token: ' my'           | Prob: 0.0915 | Perplexity: 10.9317\n",
      "    Token: ' the'          | Prob: 0.0765 | Perplexity: 13.0794\n",
      "    Token: ' it'           | Prob: 0.0514 | Perplexity: 19.4696\n",
      "    Token: ' and'          | Prob: 0.0393 | Perplexity: 25.4449\n",
      "    Token: ' one'          | Prob: 0.0378 | Perplexity: 26.4595\n",
      "    Token: ' this'         | Prob: 0.0347 | Perplexity: 28.8524\n",
      "    Token: ' something'    | Prob: 0.0249 | Perplexity: 40.1467\n",
      "    Token: ' some'         | Prob: 0.0222 | Perplexity: 45.0222\n",
      "Step 14: Chosen -> ' interactive'\n",
      "    Token: ' '             | Prob: 0.0671 | Perplexity: 14.8994\n",
      "    Token: ' interactive'  | Prob: 0.0572 | Perplexity: 17.4973\n",
      "    Token: ' app'          | Prob: 0.0544 | Perplexity: 18.3882\n",
      "    Token: ' exhibit'      | Prob: 0.0285 | Perplexity: 35.1448\n",
      "    Token: ' art'          | Prob: 0.0263 | Perplexity: 38.0703\n",
      "    Token: '\\xa0'          | Prob: 0.0169 | Perplexity: 59.2552\n",
      "    Token: ' exhibition'   | Prob: 0.0149 | Perplexity: 67.2563\n",
      "    Token: ' entire'       | Prob: 0.0131 | Perplexity: 76.2520\n",
      "    Token: ' online'       | Prob: 0.0128 | Perplexity: 77.8432\n",
      "    Token: ' experiment'   | Prob: 0.0116 | Perplexity: 86.1055\n",
      "Step 15: Chosen -> ' representations'\n",
      "    Token: ' map'          | Prob: 0.1152 | Perplexity: 8.6816\n",
      "    Token: ' museum'       | Prob: 0.0481 | Perplexity: 20.8078\n",
      "    Token: ' version'      | Prob: 0.0198 | Perplexity: 50.5765\n",
      "    Token: ' sculpture'    | Prob: 0.0163 | Perplexity: 61.4034\n",
      "    Token: ' display'      | Prob: 0.0144 | Perplexity: 69.3660\n",
      "    Token: ' exhibit'      | Prob: 0.0124 | Perplexity: 80.3948\n",
      "    Token: ' tool'         | Prob: 0.0119 | Perplexity: 84.1676\n",
      "    Token: ' interactive'  | Prob: 0.0116 | Perplexity: 86.3018\n",
      "    Token: ' game'         | Prob: 0.0105 | Perplexity: 95.2653\n",
      "    Token: ' wall'         | Prob: 0.0089 | Perplexity: 111.7629\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Set device to GPU if available, else CPU\n",
    "device = \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "print(f\"Running on: {device}\")\n",
    "\n",
    "def generate_with_top_k_stats(prompt, num_generate=10, top_k_candidates=5, model_name='gpt2'):\n",
    "    print(f\"Loading {model_name}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    \n",
    "    # MOVE MODEL TO DEVICE\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # FIX PAD TOKEN WARNING (GPT-2 specific)\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    # Encode and move inputs to device\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "    generation_stats = []\n",
    "\n",
    "    print(f\"\\nGenerating {num_generate} tokens for prompt: '{prompt}'\\n\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_generate):\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids)\n",
    "            predictions = outputs.logits\n",
    "\n",
    "            # Get logits for the last token\n",
    "            next_token_logits = predictions[0, -1, :]\n",
    "\n",
    "            # Softmax\n",
    "            probs = torch.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "            # Get Top-K\n",
    "            top_probs, top_indices = torch.topk(probs, top_k_candidates)\n",
    "\n",
    "            step_candidates = []\n",
    "            for prob, idx in zip(top_probs, top_indices):\n",
    "                token_str = tokenizer.decode([idx])\n",
    "                # Calculate perplexity (1/prob)\n",
    "                token_perplexity = math.exp(-torch.log(prob).item()) if prob > 0 else float('inf')\n",
    "                \n",
    "                step_candidates.append({\n",
    "                    \"token\": token_str,\n",
    "                    \"prob\": prob.item(),\n",
    "                    \"perplexity\": token_perplexity\n",
    "                })\n",
    "\n",
    "            # Sample next token\n",
    "            next_token_index = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Store stats\n",
    "            chosen_token_str = tokenizer.decode(next_token_index)\n",
    "            generation_stats.append({\n",
    "                \"step\": i + 1,\n",
    "                \"chosen_token\": chosen_token_str,\n",
    "                \"top_candidates\": step_candidates\n",
    "            })\n",
    "\n",
    "            # Append to input_ids\n",
    "            input_ids = torch.cat([input_ids, next_token_index.unsqueeze(0)], dim=1)\n",
    "\n",
    "    full_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "    return full_text, generation_stats\n",
    "\n",
    "# --- Usage ---\n",
    "prompt_text = \"Given that \"\n",
    "generated_text, stats = generate_with_top_k_stats(\n",
    "    prompt=prompt_text, \n",
    "    num_generate=15,      \n",
    "    top_k_candidates=10,  \n",
    "    model_name='openai-community/gpt2' \n",
    ")\n",
    "\n",
    "print(f\"Final Text: {generated_text}\\n\")\n",
    "print(\"-\" * 30)\n",
    "for stat in stats:\n",
    "    print(f\"Step {stat['step']}: Chosen -> '{stat['chosen_token']}'\")\n",
    "    for cand in stat['top_candidates']:\n",
    "        print(f\"    Token: {repr(cand['token']):<15} | Prob: {cand['prob']:.4f} | Perplexity: {cand['perplexity']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d881b58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project (3.13.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
